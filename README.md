# DEEP-MULTI-MODAL-EMOTION-RECOGNITION-FOR-ADAPTIVE-AI-SYSTEMS-IN-HUMAN-CENTERED-APPLICATIONS
Project Overview
This project presents an advanced approach to multimodal emotion recognition by integrating:

Deep Learning Models (CNN, LSTM, DNN)

Feature Fusion Techniques (Feature-level and Decision-level Fusion)

Real-time Deployment via Web Application

It aims to accurately classify human emotions (happiness, sadness, anger, fear, surprise, neutrality, etc.) by analyzing speech, text, and facial expressions, providing a robust and scalable solution for domains like mental health monitoring, customer service, education, and human-computer interaction.

Features
üîç Multimodal Emotion Detection (Speech, Text, Facial Expressions)
üß† Advanced Deep Learning Models (CNN, LSTM, Random Forest, DNN)
ü§ñ Wavelet Transform for better facial feature extraction
üìä Performance Evaluation using Confusion Matrix, Accuracy, Precision, Recall, F1-score
üåê Fully Functional Web Application (User Registration, Login, Real-time Upload and Emotion Prediction)

Tech Stack
Python

Flask (Backend Framework)

TensorFlow (Deep Learning Library)

PyTorch (for Facial Emotion Recognition Models)

Keras (Model Training)

Hugging Face Transformers (BERT for Text)

Librosa (Audio feature extraction)

Scikit-learn (Random Forest, Evaluation Metrics)

MySQL (Database for user login/registration)

HTML/CSS/JavaScript (Frontend)

Streamlit (Optional) ‚Äì For experimental UI (streamlit run src/app.py)

Contributor
Makkena Velangani Akshitha
LinkedIn - www.linkedin.com/in/makkena-velangani-akshitha-299629317
makkenavelanganiakshitha@karunya.edu.in

